---
title: "Linear Models with Regularization"
author: "Yifei Sun, Runze Cui, Chen Liang, Chenshuo Pan"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
--- 

\newpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(ISLR)
library(glmnet)
library(caret)
library(tidymodels)
library(corrplot)
library(ggplot2)
library(plotmo)
library(ggrepel)
```

Predict a baseball playerâ€™s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details.

```{r}
data(Hitters)
Hitters <- na.omit(Hitters)

set.seed(2222)
data_split <- initial_split(Hitters, prop = 0.8)

# Extract the training and test data
training_data <- training(data_split)
testing_data <- testing(data_split)
```

# 1) Using `glmnet`

## Ridge regression

```{r}
# matrix of predictors (glmnet uses input matrix)
x <- model.matrix(Salary ~ ., training_data)[,-1]
# vector of response
y <- training_data[, "Salary"]

corrplot(cor(x), method = "circle", type = "full")
```

`alpha` is the elastic net mixing parameter. `alpha=1` is the lasso penalty, and `alpha=0` the ridge penalty. `glmnet()` function standardizes the independent variables by default (The coefficients are always returned on the original scale). 

```{r}
# fit the ridge regression (alpha = 0) with a sequence of lambdas
ridge.mod <- glmnet(x = x, y = y, 
                    # standardize = TRUE,
                    alpha = 0, 
                    lambda = exp(seq(10, -5, length = 100)))
```

`coef(ridge.mod)` gives the coefficient matrix. Each column is the fit corresponding to one lambda value.

```{r}
mat.coef <- coef(ridge.mod)
dim(mat.coef)
```


### Trace plot

```{r}
# plot(ridge.mod, xvar = "lambda", label = TRUE)
plot_glmnet(ridge.mod, xvar = "rlambda", label = 19)
```

### Cross-validation

We use cross-validation to determine the optimal value of `lambda`. The two vertical lines are the for minimal MSE and 1SE rule. The 1SE rule gives the most regularized model such that error is within one standard error of the minimum.

```{r}
set.seed(2)
cv.ridge <- cv.glmnet(x, y, 
                      alpha = 0, 
                      lambda = exp(seq(10, -5, length = 100)))
# set.seed(2)
# cv.ridge <- cv.glmnet(x, y, alpha = 0, nlambda = 200)

plot(cv.ridge)
abline(h = (cv.ridge$cvm + cv.ridge$cvsd)[which.min(cv.ridge$cvm)], col = 4, lwd = 2)

# min CV MSE
cv.ridge$lambda.min
# the 1SE rule
cv.ridge$lambda.1se
```


### Coefficients of the final model

Get the coefficients of the optimal model. `s` is value of the penalty parameter `lambda` at which predictions are required.

```{r}
# extract coefficients
predict(cv.ridge, s = cv.ridge$lambda.min, type = "coefficients") 

# make prediction
head(predict(cv.ridge, newx = model.matrix(Salary ~ ., testing_data)[,-1], 
             s = "lambda.min", type = "response")) 

# predict(cv.ridge, s = "lambda.min", type = "coefficients") 
# predict(cv.ridge, s = "lambda.1se", type = "coefficients") 
# predict(ridge.mod, s = cv.ridge$lambda.min, type = "coefficients")
```

## Lasso 

The syntax is along the same line as ridge regression. Now we use `alpha = 1`.

```{r}
cv.lasso <- cv.glmnet(x, y, 
                      alpha = 1, 
                      lambda = exp(seq(6, -5, length = 100)))

cv.lasso$lambda.min
```

```{r}
plot(cv.lasso)
```

```{r}
# cv.lasso$glmnet.fit is a fitted glmnet object using the full training data
# plot(cv.lasso$glmnet.fit, xvar = "lambda", label=TRUE)
plot_glmnet(cv.lasso$glmnet.fit)
```


```{r}
predict(cv.lasso, s = "lambda.min", type = "coefficients")

head(predict(cv.lasso, newx = model.matrix(Salary ~ ., testing_data)[,-1], 
             s = "lambda.min", type = "response"))
```


# 2) Using `caret`


## Ridge regression

```{r}
ctrl1 <- trainControl(method = "cv", number = 10)

set.seed(2)
ridge.fit <- train(Salary ~ . ,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(6, 0, length=100))),
                   trControl = ctrl1)

plot(ridge.fit, xTrans = log)


ridge.fit$bestTune
```


### Why is the CV RMSE curve flat?

Mistake in caret implementation

```{r}
train_id_list <- ridge.fit$control$index

dat_dummy <- data.frame(Salary = y, x)
M <- 10
lambda.grid <- exp(seq(6, 0, length = 100))
rmse <- rmse_caret <- matrix(NA, ncol = 100, nrow = M)

for (m in 1:M)
{
  tsdata <- dat_dummy[train_id_list[[m]],] 
  vsdata <- dat_dummy[-train_id_list[[m]],] 
  
  x1 <- as.matrix(tsdata[,-1])
  y1 <- tsdata[,1]
  x2 <- as.matrix(vsdata[,-1])
  y2 <- vsdata[,1]
  
  fit <- glmnet(x1, y1, alpha = 0, 
                lambda = lambda.grid)
  
  # caret implementation did not specify lambda
  # the default grid of lambda is different from lambda.grid
  fit_caret <- glmnet(x1, y1, alpha = 0)
  
  pred <- predict(fit, newx = x2, s = lambda.grid)
  pred_caret <- predict(fit_caret, newx = x2, s = lambda.grid)
  
  rmse[m,] <- sqrt(colMeans((y2 - pred)^2))
  rmse_caret[m,] <- sqrt(colMeans((y2 - pred_caret)^2))
}

# curve from glmnet (correct)
plot(log(lambda.grid), colMeans(rmse), col = 3, xlab = "log(lambda)", ylab = "CV RMSE")
abline(v = log(lambda.grid[which.min(colMeans(rmse))]))

# caret results
points(log(ridge.fit$results$lambda), ridge.fit$results$RMSE, col = 2)

# try to reproduce caret results from scratch
points(log(lambda.grid), colMeans(rmse_caret), col = rgb(0,0,1,alpha = 0.3))


# selected lambda
lambda.grid[which.min(colMeans(rmse))]

# the corresponding CV RMSE
min(colMeans(rmse))
```

## Lasso

```{r}
set.seed(2)
lasso.fit <- train(Salary ~ .,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(6, 0, length = 100))),
                   trControl = ctrl1)
plot(lasso.fit, xTrans = log)

lasso.fit$bestTune

# coefficients in the final model
coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)
```

## Elastic net

```{r}
set.seed(2)
enet.fit <- train(Salary ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(6, 0, length = 100))),
                  trControl = ctrl1)
enet.fit$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)

# coefficients in the final model
coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
```

## Comparing different models

```{r, fig.width=5}
set.seed(2)
lm.fit <- train(Salary ~ .,
                data = training_data,
                method = "lm",
                trControl = ctrl1)

resamp <- resamples(list(enet = enet.fit, lasso = lasso.fit, ridge = ridge.fit, lm = lm.fit))

summary(resamp)

parallelplot(resamp, metric = "RMSE")
# bwplot(resamp, metric = "RMSE")
```

## Prediction

```{r}
enet.pred <- predict(enet.fit, newdata = testing_data)
# test error
mean((enet.pred - testing_data[, "Salary"])^2)
```


# 3) Using `tidymodels` (optional)

## Ridge regression

```{r}
# 3.1 setup the resampling method
set.seed(2)
cv_folds <- vfold_cv(training_data, v = 10) 

# 3.2 model specification for ridge regression
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) |> # mixture = 0 for ridge regression
  set_engine("glmnet") |> 
  set_mode("regression")

# ridge_spec |> extract_parameter_dials("penalty")

# 3.3 grid of tuning parameters (log scale)
ridge_grid_set <- parameters(penalty(range = c(-2, 5), trans = log_trans()))
ridge_grid <- grid_regular(ridge_grid_set, levels = 100)

# 3.4 set up the workflow
ridge_workflow <- workflow() |>
  add_model(ridge_spec) |>
  add_formula(Salary ~ .)

# 3.5 model tuning
ridge_tune <- tune_grid(
  ridge_workflow,
  resamples = cv_folds,
  grid = ridge_grid,
  control = control_resamples(extract = extract_fit_parsnip, save_pred = TRUE)
)

# CV plot
autoplot(ridge_tune, metric = "rmse") 
```

### Why is the CV RMSE curve flat?

```{r}
set.seed(2)
cvSplits <- vfold_cv(data.frame(Salary = y, x), v = 10) 
M <- 10
lambda.grid <- exp(seq(5, -2, length = 100))
rmse_r <- rmse_r_tm <- matrix(NA, ncol = 100, nrow = M)
for (m in 1:M)
{
  tsdata <- analysis(cvSplits[[1]][[m]]) 
  vsdata <- assessment(cvSplits[[1]][[m]]) 
  
  x1 <- as.matrix(tsdata[,-1])
  y1 <- tsdata[,1]
  x2 <- as.matrix(vsdata[,-1])
  y2 <- vsdata[,1]
  
  fit <- glmnet(x1, y1, alpha = 0, 
                lambda = lambda.grid)
  
  # tidymodels/caret implementation did not specify lambda
  # the default grid of lambda is different from lambda.grid
  fit_tm <- glmnet(x1, y1, alpha = 0)
  
  pred <- predict(fit, newx = x2, s = lambda.grid)
  pred_tm <- predict(fit_tm, newx = x2, s = lambda.grid)
  
  rmse_r[m,] <- sqrt(colMeans((y2 - pred)^2))
  rmse_r_tm[m,] <- sqrt(colMeans((y2 - pred_tm)^2))
}


# curve from glmnet (correct)
plot(log(lambda.grid), colMeans(rmse_r), col = 3, xlab = "log(lambda)", ylab = "CV RMSE")
abline(v = log(lambda.grid[which.min(colMeans(rmse_r))]))

# curve from tidymodels
points(seq(-2, 5, length = 100), 
       (ridge_tune |> 
          collect_metrics() |> 
          filter(.metric == "rmse") |>
          select(mean))[[1]],
       col = 2)

# try to reproduce tidymodels results from scratch
points(log(lambda.grid), colMeans(rmse_r_tm), col = rgb(0, 0, 1, alpha = 0.3))
```

```{r}
# 3.6 tuning parameter selection
ridge_best <- lambda.grid[which.min(colMeans(rmse_r))]# select_best(ridge_tune, metric = "rmse") 


# 3.7 update the model with the selected lambda
final_ridge_spec <- ridge_spec |> 
  update(penalty = ridge_best)

# 3.8 fit the final model 
ridge_fit <- fit(final_ridge_spec, formula = Salary ~ ., data = training_data)

# coefficients of the final model
ridge_model <- extract_fit_engine(ridge_fit)
coef(ridge_model, s = ridge_best)
```


## Lasso

```{r}
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) |> # mixture = 1 for lasso regression
  set_engine("glmnet") |> 
  set_mode("regression")

# lasso_spec |> extract_parameter_dials("penalty")

lasso_grid_set <- parameters(penalty(range = c(-3, 5), trans = log_trans()))
lasso_grid <- grid_regular(lasso_grid_set, levels = 100)

lasso_workflow <- workflow() |>
  add_model(lasso_spec) |>
  add_formula(Salary ~ .)

lasso_tune <- tune_grid(
  lasso_workflow,
  resamples = cv_folds,
  grid = lasso_grid
)

autoplot(lasso_tune, metric = "rmse")

lasso_best <- select_best(lasso_tune, metric = "rmse") 

final_lasso_spec <- lasso_spec |> 
  update(penalty = lasso_best$penalty)

lasso_fit <- fit(final_lasso_spec, formula = Salary ~ ., data = training_data)

lasso_model <- extract_fit_engine(lasso_fit)
coef(lasso_model, s = lasso_best$penalty)
```

## Elastic net

```{r}
enet_spec <- linear_reg(penalty = tune(), mixture = tune()) |> 
  set_engine("glmnet") |> 
  set_mode("regression")

# enet_spec |> extract_parameter_dials("mixture")
# enet_spec |> extract_parameter_dials("mixture")

enet_grid_set <- parameters(penalty(range = c(-3, 5), trans = log_trans()),
                            mixture(range = c(0, 1)))
enet_grid <- grid_regular(enet_grid_set, levels = c(100, 21))



enet_workflow <- workflow() |>
  add_model(enet_spec) |>
  add_formula(Salary ~ .)

enet_tune <- tune_grid(
  enet_workflow,
  resamples = cv_folds,
  grid = enet_grid
)

autoplot(enet_tune, metric = "rmse") + 
  theme(legend.position = "top") +
  labs(color = "Mixing Percentage\n(Alpha Values)") 

enet_best <- select_best(enet_tune, metric = "rmse") 

final_enet_spec <- enet_spec |> 
  update(penalty = enet_best$penalty, mixture = enet_best$mixture)

enet_fit <- fit(final_enet_spec, formula = Salary ~ ., data = training_data)

# Get coefficients
enet_model <- extract_fit_engine(enet_fit)
coef(enet_model, s = enet_best$penalty)
```

## Comparing different models

```{r}
lm_spec <- linear_reg() |> 
  set_engine("lm") |> 
  set_mode("regression")

model_compare <- workflow_set(preproc = list(Salary ~ .),
                              models = list(lm = lm_spec, 
                                            lasso = final_lasso_spec,
                                            ridge = final_ridge_spec,
                                            enet = final_enet_spec)) |> 
  workflow_map(resamples = cv_folds) 

autoplot(model_compare, metric = "rmse") +
  geom_text_repel(aes(label = wflow_id, color = wflow_id), 
                  nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")
```

## Prediction

```{r}
enet_pred <- predict(enet_fit, new_data = testing_data)

# Calculate test RMSE
sqrt(mean((enet_pred[[1]] - testing_data$Salary)^2))
```


